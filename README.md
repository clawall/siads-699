# SIADS-699:  Team 12: Effects of social media from influential people into the Stock Market
Shared repository for team 12: `This is Fine` project on SIADS-699.

This repository contains a subset of available packages on the Coursera repository and is meant to ease the collaboration between the team as well as help track our progress.

## Team members
- Roy Spencer - roylt
- César Silveira - clawall
- Darienne Sautter - dsautter

## Pre-requisities
- Python 3
- Python virtualenv

### About Virtualenv
Virtualenv is an amazing tool that helps us isolate our environment.  By containing our libraries within its folder structure, we avoid the use of libraries installed on our local OS and can easily share the exact environment amongst the team members.

You can have as many virtual environments as you want but it's a good idea to use a single environment for a specific project.  Each time we add a new library, we need to save it's name (following the pattern`library-name==version`) on a file called `requirements.txt`.  If we ever need to recreate an environment, we can just use the commands specified on the next session to create it.

Please refer to its [documentation](https://docs.python.org/3/library/venv.html) for more instructions and a how-to-use guide.

## Installation
By having Python 3 and Virtualenv installed (please refer to the guides for your operating system within their respective docs) we just need to start our virtual environment and install the libraries contained within our requirements file.

Warning:  The first command on the shell bellow assumes that your Python3 binary is called "python3" and is accessible on the OS path.  Please update this accordingly.

```console
# 1. Creates a new environment with the python executable "python3" on the folder "./.env"
virtualenv -p python3 .env

# 2. Activates the newly created virtual environment
source .env/bin/activate

# 3. Installs the libraries indicated on the "requirements.txt" file
pip install -r requirements.txt

# 4. Running parts of the project:
## 4.1. If you want to edit/run the Notebooks:  Starts Jupyter Lab so we can work (it will be opened on a browser)
OPENAI_API_KEY=YOUR_OPENAI_API_KEY jupyter lab

## 4.2. If you want to run the ML Pipeline and create fresh CSV files to train the Models
make
```

## Folder structure
- ./ --> Project settings
- ./notebooks/ --> Notebooks for the project
- ./src --> Python files (utilities)
- ./test --> Unit tests

## Data sources
### Data Citation
Wharton Research Data Services. "WRDS" wrds.wharton.upenn.edu, accessed 2025-04-07.

### Data Access Statement
This project uses multiple sources of data, each with its own sources.  Third-party data needed for it is not persisted on Github so to acquire all that is needed, follow these steps:

#### Twitter/X data
Data was collected from the site Polititweet.org.  As stated in their site:
```
© 2025 Politiwatch. Tweets and other media belong to their indicated owners; all other materials are licensed CC-BY-SA. If you use PolitiTweet professionally, please feel free to let us know. Note that PolitiTweet stopped archiving new tweets on April 3, 2023, when Twitter disabled our API access.
```

A web scraper was built to collect this data and its available as a Makefile target (see section "Machine Learning Pipeline").  Data will be collected and generated by running the targets `./data/03_raw_df.csv` or `all`.

#### TAQ data
TAQ dataset was collected from Wharton Research Data Service (WRDS) as stated in the section "Data Citation" above.  A manual query for people with read access is required:

1. Navigate to the site, log in;
2. On the menu, select "Get Data" > "TAQ";
3. Select the option "Millisecond Trade and Quote - "Daily Product". 2003 - present, updated daily.";
4. Repeat the steps below from year 2010 to 2020:
    * On the "TAQ - Millisecond Consolidated Trades" query builder, select the desired year;
    * On "Autocomplete", ensure that "SYMBOL" is checked and then add all tickers present in the file `./data/04_people_stock_link.csv` (refer to the full list below);
    * Select the following variables for the resultset:  `Date of trade (DATE)`, `Time of Trade or Quote with milliseconds (HHMMSSXXX) (TIME_M)`, ` Security symbol root (SYM_ROOT)`,`Security symbol suffix (SYM_SUFFIX)`, `Volume of trade (SIZE)`, `Price of trade (PRICE)`;
    * Run the query;
    * When you receive the results, download it;
    * Decompress this file in the `./data/taq_raw` folder;
    * After you collect all 10 years of data, run the Makefile target `./data/taq/.stamp` -- it will convert your CSV files into compressed and split parquet files;

Here's the ticker list used on this project, for reference:
```BRK.B AAPL BAC KO PYPL SOFI SQ SHOP AFRM DWAC LVS MGM WYNN FOX WFC JPM C GS TSLA CHPT EVGO NIO RIVN MRNA AMZN WMT MSFT META COIN RBLX ABNB PINS MSTR LMT PFE JNJ XOM AAL LCID WKHS NOC MMS BAH LDOS CAT DELL NVAX BA GME AMC BTC-USD DOGE-USD NOK F SNAP NFLX BYND DAL SPCE PLTR SPOT NKE PEP HD TGT CVX NVDA AMD GOOGL TWTR ADBE DIS GM GE ATVI TTWO ZM TSM```

#### VIX data
VIX data was collected from [cboe.org](https://www.cboe.com/tradable_products/vix/vix_historical_data/) and is made available by running the Mafile goal `./data/VIX1.csv`.  Please refer to the Machile Learning Pipeline described below.

## Machine Learning Pipeline
A pipeline was constructed to prepare and clean data in preparation for the Models to be trained.  It can be triggered by running the `make` command and will run a series of targets:
1. Read from the list of Twitter handles, scrapes Politiweet and conver them to a list of user IDs;
2. Scrapes Politiweet for all tweets from the list of user IDs;
3. Splits the CSV file generated in the previous steps into smaller chunks so they can be persisted in GitHub;
4. Cleans up and merges the splitted CSV files into a single one to be read and processed in Notebooks;
5. Downloads and cleans up VIX data;

### Extra target
There's also an extra step that's not part of the main Pipeline, which converts the really large TAQ files (about 60Gb per file, corresponding to 1 year) into splitted compressed parquet files that can be stored in GitHub while also allowing us to load them directly into Pandas DataFrames.

## Running Unit Tests  

This project includes unit tests to ensure the correctness of utility functions. The tests are written using `pytest` and can be found in the `tests/` directory.  

### **Prerequisites**  
Make sure you have test libraries installed. If it's not installed, you can add it using:  

```bash
pip install -r requirements-test.txt
```

### Running All Tests
To run all unit tests, navigate to the root directory of the project and execute:

```bash
pytest
```

### Running Tests for a Specific File
If you want to run tests from a specific file, use:

```bash
pytest tests/test_utils.py
````

### Running a Specific Test Case
To run an individual test case, use the -k option with a part of the test name:

```bash
pytest -k "test_basic_functionality"
```

### Running Tests with Detailed Output
For more verbose output, use the -v flag:

```bash
pytest -v
```

### Checking Test Coverage
If you want to check test coverage using pytest-cov, install it first:

```bash
pip install pytest-cov
```

Then run:

```bash
pytest --cov=src
```
This will display a coverage report for the src/ directory.

## Code Style and Linting

We use `pylint` to enforce PEP 8 coding standards in this project.

### Installing the Linter
Ensure you have `pylint` installed. If not, install it using:
```bash
pip install -r requirements-test.txt
```

### Running the Linter
To check for PEP 8 violations, run the following command in the project's root directory:

```bash
pylint --fail-under=8 $(git ls-files 'src/*.py')
```
This will scan all Python files inside the src/ directory.

### Ignoring Files and Folders
If you want to ignore specific files or folders, you can configure .pylintrc in the project's root.

Example `.pylintrc` Configuration:
```ini
[MAIN]
ignore-paths=src/.ipynb_checkpoints
ignore-patterns=^\.#
```
This will exclude the notebooks/, migrations/, tests/, and some_folder/ directories from linting.

## GitHub Actions: Lint & Test Workflow
This project uses GitHub Actions to automate code quality checks. The workflow includes two jobs:

Lint - Runs pylint to check for code quality and formatting issues.
Test - Runs unit tests using pytest.

### Workflow Configuration
The GitHub Actions workflow is defined in .github/workflows/pylint.yml and runs automatically on every push and pull request.

### How It Works
- On push or pull request, GitHub Actions:
1. Installs Python dependencies.
2. Runs Pylint (--fail-under=8 ensures a minimum score of 8).
3. If linting succeeds, it proceeds to unit tests with pytest.

### Passing Criteria
- Linting must pass (score ≥ 8).
- Tests must pass with pytest.
